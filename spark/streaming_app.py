"""
Spark Streaming Application
‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GPS ‡∏à‡∏≤‡∏Å Kafka ‚Üí ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‚Üí ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Elasticsearch
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, struct
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, FloatType

def main():
    # ============================================
    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á SparkSession
    # ============================================
    spark = SparkSession.builder \
        .appName("HybridTransitStreaming") \
        .config("spark.jars.packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,"
                "org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    print("‚úÖ Spark Session created.")

    # ============================================
    # 2. Schema ‡∏Ç‡∏≠‡∏á JSON ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka
    # ============================================
    schema = StructType([
        StructField("bus_id", StringType(), True),
        StructField("route", StringType(), True),
        StructField("timestamp", TimestampType(), True),
        StructField("lat", DoubleType(), True),
        StructField("lon", DoubleType(), True),
        StructField("speed_kmh", FloatType(), True)
    ])

    # ============================================
    # 3. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka
    # ============================================
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:29092") \
        .option("subscribe", "bus-locations") \
        .option("startingOffsets", "latest") \
        .load()

    print("‚úÖ Kafka stream DataFrame created.")
    kafka_df.printSchema()

    # ============================================
    # 4. ‡πÅ‡∏õ‡∏•‡∏á JSON ‡πÄ‡∏õ‡πá‡∏ô columns
    # ============================================
    processed_df = kafka_df \
        .selectExpr("CAST(value AS STRING)") \
        .select(from_json(col("value"), schema).alias("data")) \
        .select("data.*")

    print("‚úÖ DataFrame with parsed JSON created.")
    processed_df.printSchema()

    # ============================================
    # 5. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Elasticsearch
    # ============================================
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á location field ‡πÄ‡∏õ‡πá‡∏ô struct ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö geo_point
    es_df = processed_df.withColumn(
        "location", 
        struct(col("lat"), col("lon"))
    )
    
    print("‚úÖ Elasticsearch DataFrame created.")
    es_df.printSchema()

    # ============================================
    # 6. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á Elasticsearch
    # ============================================
    query_es = es_df.writeStream \
        .format("org.elasticsearch.spark.sql") \
        .option("es.resource", "bus-locations") \
        .option("es.nodes", "elasticsearch") \
        .option("es.port", "9200") \
        .option("es.nodes.wan.only", "false") \
        .option("es.mapping.id", "bus_id") \
        .option("checkpointLocation", "/tmp/spark-checkpoint-es") \
        .start()

    # ============================================
    # 7. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏≠‡∏≠‡∏Å console ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug
    # ============================================
    query_console = es_df.writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", "false") \
        .start()
    
    print("‚úÖ Streaming queries started.")
    print("üìä ‡∏£‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka...")
    print("üí° ‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏±‡∏ô: cd producer && python bus_producer.py")

    # ============================================
    # 8. ‡πÉ‡∏´‡πâ Spark ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ
    # ============================================
    spark.streams.awaitAnyTermination()


if __name__ == "__main__":
    main()